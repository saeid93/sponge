{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Server with Minio but no Seldon\n",
    " [source](https://thenewstack.io/deploy-nvidia-triton-inference-server-with-minio-as-model-store/?fr=)\n",
    " use the former notebook to generate model variants\n",
    " ### single node triton server with load test\n",
    " 1. Loading to and from minio workflow (multiple models)\n",
    " 2. Getting models from [timm](https://timm.fast.ai/)\n",
    " 3. Making the node\n",
    " 4. Load and unload models operations\n",
    " 5. Monitoring\n",
    " 6. Naive load test\n",
    " 7. TODO Add language models from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('sudo umount -l ~/my_mounting_point')\n",
    "os.system('cc-cloudfuse mount ~/my_mounting_point')\n",
    " \n",
    "data_folder_path = '/home/cc/my_mounting_point/datasets'\n",
    "dataset_folder_path = os.path.join(\n",
    "    data_folder_path, 'ILSVRC/Data/DET/test'\n",
    ")\n",
    "classes_file_path = os.path.join(\n",
    "    data_folder_path, 'imagenet_classes.txt'\n",
    ")\n",
    " \n",
    "image_names = os.listdir(dataset_folder_path)\n",
    "image_names.sort()\n",
    "with open(classes_file_path) as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "def image_loader(folder_path, image_name):\n",
    "    image = Image.open(\n",
    "        os.path.join(folder_path, image_name))\n",
    "    # if there was a need to filter out only color images\n",
    "    # if image.mode == 'RGB':\n",
    "    #     pass\n",
    "    return image\n",
    "num_loaded_images = 4\n",
    "images = {\n",
    "    image_name: image_loader(\n",
    "        dataset_folder_path, image_name) for image_name in image_names[\n",
    "            :num_loaded_images]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    " \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")])\n",
    " \n",
    "batch = torch.stack(list(map(lambda a: transform(a), list(images.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'harp', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'resnet50'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.eval()\n",
    "torch_output = model(batch)\n",
    "torch_output = torch.nn.functional.softmax(torch_output, dim=1) * 100\n",
    "torch_output = torch_output.detach().numpy()\n",
    "torch_output = torch_output.argmax(axis=1)\n",
    "torch_class = np.array(classes)[torch_output]\n",
    "torch_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/resnet50/1/model.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=14'>15</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=15'>16</a>\u001b[0m \u001b[39m# Invoke export\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=16'>17</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=17'>18</a>\u001b[0m     model, dummy_input,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=18'>19</a>\u001b[0m     model_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=19'>20</a>\u001b[0m     input_names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=20'>21</a>\u001b[0m     output_names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=21'>22</a>\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m : {\u001b[39m0\u001b[39;49m : \u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m},    \u001b[39m# variable length axes\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000006vscode-remote?line=22'>23</a>\u001b[0m                   \u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m : {\u001b[39m0\u001b[39;49m : \u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m}})\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/torch/onnx/__init__.py:350\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mExports a model into ONNX format. If ``model`` is not a\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m:class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    model to the file ``f`` even if this is raised.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m--> 350\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m    351\u001b[0m     model,\n\u001b[1;32m    352\u001b[0m     args,\n\u001b[1;32m    353\u001b[0m     f,\n\u001b[1;32m    354\u001b[0m     export_params,\n\u001b[1;32m    355\u001b[0m     verbose,\n\u001b[1;32m    356\u001b[0m     training,\n\u001b[1;32m    357\u001b[0m     input_names,\n\u001b[1;32m    358\u001b[0m     output_names,\n\u001b[1;32m    359\u001b[0m     operator_export_type,\n\u001b[1;32m    360\u001b[0m     opset_version,\n\u001b[1;32m    361\u001b[0m     do_constant_folding,\n\u001b[1;32m    362\u001b[0m     dynamic_axes,\n\u001b[1;32m    363\u001b[0m     keep_initializers_as_inputs,\n\u001b[1;32m    364\u001b[0m     custom_opsets,\n\u001b[1;32m    365\u001b[0m     export_modules_as_functions,\n\u001b[1;32m    366\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/torch/onnx/utils.py:163\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    146\u001b[0m     model,\n\u001b[1;32m    147\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     export_modules_as_functions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m ):\n\u001b[0;32m--> 163\u001b[0m     _export(\n\u001b[1;32m    164\u001b[0m         model,\n\u001b[1;32m    165\u001b[0m         args,\n\u001b[1;32m    166\u001b[0m         f,\n\u001b[1;32m    167\u001b[0m         export_params,\n\u001b[1;32m    168\u001b[0m         verbose,\n\u001b[1;32m    169\u001b[0m         training,\n\u001b[1;32m    170\u001b[0m         input_names,\n\u001b[1;32m    171\u001b[0m         output_names,\n\u001b[1;32m    172\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    173\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    174\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    175\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    176\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    177\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    178\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/torch/onnx/utils.py:1148\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m export_type \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE:\n\u001b[1;32m   1147\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(export_map) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1148\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mserialization\u001b[39m.\u001b[39;49m_open_file_like(f, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m   1149\u001b[0m         opened_file\u001b[39m.\u001b[39mwrite(proto)\n\u001b[1;32m   1150\u001b[0m \u001b[39melif\u001b[39;00m export_type \u001b[39min\u001b[39;00m [\n\u001b[1;32m   1151\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mZIP_ARCHIVE,\n\u001b[1;32m   1152\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mCOMPRESSED_ZIP_ARCHIVE,\n\u001b[1;32m   1153\u001b[0m ]:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/resnet50/1/model.onnx'"
     ]
    }
   ],
   "source": [
    "# save the onnx model\n",
    "import torch.onnx\n",
    " \n",
    "model_variant = 1\n",
    "model_dir = os.path.join(\n",
    "    'models',\n",
    "    model_name,\n",
    "    str(model_variant))\n",
    "model_path = os.path.join(model_dir, 'model.onnx')\n",
    "if 'models' not in os.listdir(\"./\"):\n",
    "    os.makedirs(model_dir)\n",
    "# Standard ImageNet input - 3 channels, 224x224,\n",
    "# values don't matter as we care about network structure.\n",
    "# But they can also be real inputs.\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Invoke export\n",
    "torch.onnx.export(\n",
    "    model, dummy_input,\n",
    "    model_path,\n",
    "    input_names = ['input'],\n",
    "    output_names = ['output'],\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                  'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lion', 'studio_couch', 'slot', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use onnx model\n",
    "import onnx\n",
    "import onnxruntime\n",
    " \n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    " \n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    os.path.join(model_dir, \"model.onnx\"),\n",
    "    providers=['CPUExecutionProvider'])\n",
    "onnx_output = ort_session.run(None, {'input': batch.numpy()})\n",
    "onnx_output = torch.nn.functional.softmax(torch.tensor(onnx_output), dim=1)[0] * 100\n",
    "onnx_output = onnx_output.detach().numpy()\n",
    "onnx_output = onnx_output.argmax(axis=1)\n",
    "onnx_class = np.array(classes)[onnx_output]\n",
    "onnx_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# TODO find out why slightly different\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(onnx_output \u001b[39m==\u001b[39m torch_output)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000009vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(onnx_class)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO find out why slightly different\n",
    " \n",
    "assert np.all(onnx_output == torch_output)\n",
    "print(onnx_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/resnet50/config.pbtxt\n",
    "name: \"resnet50\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size : 100\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "d4be65a38ca32d0685d70b03a379071b3f2f666f3aab9173a6af868e34422ed3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VERSION='22.05'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Client Examples\n",
    " \n",
    " [examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\n",
    " \n",
    " [grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\n",
    " \n",
    " [http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Python Client Examples\n",
      " \n",
      " [examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\n",
      " \n",
      " [grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\n",
      " \n",
      " [http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "print(    \"### Python Client Examples\\n\",\n",
    "    \"\\n\",\n",
    "    \"[examples](https://github.com/triton-inference-server/client/tree/main/src/python/examples)\\n\",\n",
    "    \"\\n\",\n",
    "    \"[grpc](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/grpc/__init__.py)\\n\",\n",
    "    \"\\n\",\n",
    "    \"[http](https://github.com/triton-inference-server/client/blob/main/src/python/library/tritonclient/http/__init__.py)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "\n",
    "model_name = \"resnet50\"\n",
    "inputs = []\n",
    "inputs.append(\n",
    "    httpclient.InferInput(\n",
    "        name=\"input\", shape=batch.shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch.numpy(), binary_data=False)\n",
    " \n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(name=\"output\"))\n",
    " \n",
    "result = triton_client.infer(\n",
    "    model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    "try:\n",
    "    triton_client = grpcclient.InferenceServerClient(\n",
    "        url='localhost:8001', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet50\"\n",
    " \n",
    "inputs = []\n",
    "inputs.append(\n",
    "    grpcclient.InferInput(name=\"input\", shape=batch.shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch.numpy())\n",
    "outputs = []\n",
    "outputs.append(grpcclient.InferRequestedOutput(name=\"output\"))\n",
    "\n",
    "result = triton_client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"resnet50\"\n",
      "platform: \"onnxruntime_onnx\"\n",
      "max_batch_size: 100\n",
      "input [\n",
      "  {\n",
      "    name: \"input\"\n",
      "    data_type: TYPE_FP32\n",
      "    format: FORMAT_NCHW\n",
      "    dims: [ 3, 224, 224 ]\n",
      "  }\n",
      "]\n",
      "output [\n",
      "  {\n",
      "    name: \"output\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [ 1000 ]\n",
      "  }\n",
      "]\n",
      "version_policy: { all { }}\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def config_builder(\n",
    "  name: str, platform: str, max_batch_size: int,\n",
    "  ):\n",
    "  config = (f\"name: \\\"{name}\\\"\\n\"\n",
    "            f\"platform: \\\"{platform}\\\"\\n\"\n",
    "            f\"max_batch_size: {str(max_batch_size)}\")\n",
    "  common_config=\"\"\"\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "]\n",
    "version_policy: { all { }}\n",
    "  \"\"\"\n",
    "  return config + common_config\n",
    " \n",
    "print(config_builder('resnet50', 'onnxruntime_onnx', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'models/resnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=51'>52</a>\u001b[0m         generate_model_variants(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=52'>53</a>\u001b[0m             model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=53'>54</a>\u001b[0m             versions\u001b[39m=\u001b[39mversion\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=54'>55</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=56'>57</a>\u001b[0m \u001b[39m# read these from json/yamls build with a proper config builder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=57'>58</a>\u001b[0m model_generator(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=58'>59</a>\u001b[0m     model_names \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mresnet\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mxception\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=59'>60</a>\u001b[0m     versions \u001b[39m=\u001b[39;49m [[\u001b[39m'\u001b[39;49m\u001b[39m18\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m34\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m101\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m41\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m65\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m71\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=60'>61</a>\u001b[0m )\n",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 18\u001b[0m in \u001b[0;36mmodel_generator\u001b[0;34m(model_names, versions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=45'>46</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(model_names) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(versions),\\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlength modes list \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not match versions list \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mfromat(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=47'>48</a>\u001b[0m         \u001b[39mlen\u001b[39m(model_names),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=48'>49</a>\u001b[0m         \u001b[39mlen\u001b[39m(versions)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=49'>50</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name, version \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(model_names, versions):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=51'>52</a>\u001b[0m     generate_model_variants(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=52'>53</a>\u001b[0m         model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=53'>54</a>\u001b[0m         versions\u001b[39m=\u001b[39;49mversion\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=54'>55</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 18\u001b[0m in \u001b[0;36mgenerate_model_variants\u001b[0;34m(model_name, versions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=10'>11</a>\u001b[0m config_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=11'>12</a>\u001b[0m     model_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mconfig.pbtxt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=13'>14</a>\u001b[0m \u001b[39m# if 'models' not in os.listdir(\"./\"):\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=14'>15</a>\u001b[0m os\u001b[39m.\u001b[39;49mmakedirs(model_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=15'>16</a>\u001b[0m config \u001b[39m=\u001b[39m config_builder(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=16'>17</a>\u001b[0m     name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=17'>18</a>\u001b[0m     platform\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39monnxruntime_onnx\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=18'>19</a>\u001b[0m     max_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000017vscode-remote?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(config_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'models/resnet'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    " \n",
    "def generate_model_variants(model_name: str = 'resnet',\n",
    "    versions: list = ['18', '34', '101']):\n",
    "    # model name\n",
    "    timm_models = timm.list_models(model_name+'*', pretrained=True)\n",
    "    model_path = os.path.join(\n",
    "        'models',\n",
    "        model_name,\n",
    "    )\n",
    "    config_path = os.path.join(\n",
    "        model_path,\n",
    "        'config.pbtxt')\n",
    "    # if 'models' not in os.listdir(\"./\"):\n",
    "    os.makedirs(model_path)\n",
    "    config = config_builder(\n",
    "        name=model_name,\n",
    "        platform='onnxruntime_onnx',\n",
    "        max_batch_size=100)\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config)\n",
    "    for variant_id, model_variant in enumerate(versions):\n",
    "        model_full_name = model_name + model_variant\n",
    "        if not model_full_name in timm_models:\n",
    "            raise ValueError(\n",
    "                f\"Model {model_full_name} does not exist\"\n",
    "            )\n",
    "        model = timm.create_model(model_full_name, pretrained=True)\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        model_variant_dir = os.path.join(model_path, str(variant_id+1))\n",
    "        model_variant_path = os.path.join(model_variant_dir, 'model.onnx')\n",
    "        # if 'models' not in os.listdir(\"./\"):\n",
    "        os.makedirs(model_variant_dir)\n",
    "        torch.onnx.export(\n",
    "            model, dummy_input,\n",
    "            model_variant_path,\n",
    "            input_names = ['input'],\n",
    "            output_names = ['output'],\n",
    "            dynamic_axes={'input' : {0 : 'batch_size'},\n",
    "                          'output' : {0 : 'batch_size'}})\n",
    " \n",
    "def model_generator(\n",
    "    model_names: List[str],\n",
    "    versions: List[List[str]]):\n",
    "    assert len(model_names) == len(versions),\\\n",
    "        \"length modes list {} does not match versions list {}\".fromat(\n",
    "            len(model_names),\n",
    "            len(versions)\n",
    "        )\n",
    "    for model_name, version in zip(model_names, versions):\n",
    "        generate_model_variants(\n",
    "            model_name=model_name,\n",
    "            versions=version\n",
    "        )\n",
    " \n",
    "# read these from json/yamls build with a proper config builder\n",
    "model_generator(\n",
    "    model_names = ['resnet', 'xception'],\n",
    "    versions = [['18', '34', '101'], ['', '41', '65', '71']]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "3cdca3d3cde8c784ee730432627a571b01e65fe357866cda02b01895f874fe71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate models\n",
    "VERSION='22.05'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\")\n",
    "# print(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "#       f\" -v {os.getcwd()}/models:/models \"\n",
    "#       f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "#       \" tritonserver --strict-model-config=false --model-repository=/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to multi-models\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    " \n",
    "model_version = \"1\"\n",
    " \n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet\"\n",
    "\n",
    "inputs = []\n",
    "inputs.append(\n",
    "    httpclient.InferInput(name=\"input\", shape=batch[0:4].shape, datatype=\"FP32\")\n",
    ")\n",
    "inputs[0].set_data_from_numpy(batch[0:4].numpy(), binary_data=False)\n",
    " \n",
    "outputs = []\n",
    "outputs.append(httpclient.InferRequestedOutput(name=\"output\"))\n",
    "\n",
    "result = triton_client.infer(\n",
    "    model_name=model_name,\n",
    "    model_version=model_version, # different form the older model\n",
    "    inputs=inputs, outputs=outputs)\n",
    "triton_client.close()\n",
    "# result.get_response()\n",
    "triton_output = result.as_numpy('output')\n",
    "triton_output = torch.nn.functional.softmax(\n",
    "    torch.tensor(triton_output), dim=1) * 100\n",
    "triton_output = triton_output.detach().numpy()\n",
    "triton_output = triton_output.argmax(axis=1)\n",
    "triton_class = np.array(classes)[triton_output]\n",
    "print(triton_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cougar', 'comic_book', 'iron', 'goldfish'], dtype='<U30')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.05-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:a85daa2907f46e70b3782818a0331df62d9b4e0b1f15f1530b2a52c8c782d46d\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "nvcr.io/nvidia/tritonserver:22.05-py3\n",
      "3ef7f53fca88af23a4ff9aab2513e81bc8de7b8bee6c6f4c67e93822d6719b3e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate models\n",
    "VERSION='22.05'\n",
    "MODEL_MANAGEMENT = 'explicit'\n",
    "os.system(f\"docker pull nvcr.io/nvidia/tritonserver:{VERSION}-py3\")\n",
    "# add --gpus=<number of gpus> on gpu machines\n",
    "# add -d to run at background and going to the next cell\n",
    "os.system(\"docker run --rm -d -p8000:8000 -p8001:8001 -p8002:8002\"\n",
    "          f\" -v {os.getcwd()}/models:/models \"\n",
    "          f\"nvcr.io/nvidia/tritonserver:{VERSION}-py3\"\n",
    "          \" tritonserver --model-repository=/models\"\n",
    "          f\" --model-control-mode={MODEL_MANAGEMENT} --load-model=*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------active models--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '345'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n",
      "--------------------unloading model: resnet--------------------\n",
      "\n",
      "POST /v2/repository/models/resnet/unload, headers None\n",
      "{\"parameters\":{\"unload_dependents\":false}}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'resnet'\n",
      "None\n",
      "--------------------active models after unloading--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '357'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"UNLOADING\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"UNLOADING\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"UNLOADING\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'UNLOADING'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'UNLOADING'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'UNLOADING'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n",
      "--------------------load model: resnet--------------------\n",
      "\n",
      "POST /v2/repository/models/resnet/load, headers None\n",
      "{}\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '0'}>\n",
      "Loaded model 'resnet'\n",
      "None\n",
      "--------------------active models after loading back--------------------\n",
      "\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '345'}>\n",
      "bytearray(b'[{\"name\":\"resnet\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"resnet\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"2\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"3\",\"state\":\"READY\"},{\"name\":\"xception\",\"version\":\"4\",\"state\":\"READY\"}]')\n",
      "{'name': 'resnet', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'resnet', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '1', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '2', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '3', 'state': 'READY'}\n",
      "{'name': 'xception', 'version': '4', 'state': 'READY'}\n"
     ]
    }
   ],
   "source": [
    "# see \n",
    "# https://github.com/triton-inference-server/client/blob/main/src/python/examples/simple_http_model_control.py\n",
    "# https://github.com/triton-inference-server/server/blob/main/docs/model_management.md\n",
    "# https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md\n",
    " \n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    " \n",
    " \n",
    "try:\n",
    "    triton_client = httpclient.InferenceServerClient(\n",
    "        url='localhost:8000', verbose=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "     print(\"context creation failed: \" + str(e))\n",
    "model_name = \"resnet\"\n",
    " \n",
    " \n",
    "print(20*'-' + 'active models' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    " \n",
    "print(20*'-' + f'unloading model: {model_name}' + 20*'-' + '\\n')\n",
    "print(triton_client.unload_model(model_name))\n",
    " \n",
    "print(20*'-' + 'active models after unloading' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    " \n",
    "print(20*'-' + f'load model: {model_name}' + 20*'-' + '\\n')\n",
    "print(triton_client.load_model(model_name))\n",
    " \n",
    "print(20*'-' + 'active models after loading back' + 20*'-' + '\\n')\n",
    "print(*triton_client.get_model_repository_index(), sep='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/minio-seldon`.\u001b[0m\n",
      "...fig.pbtxt:  800.38 MiB / 800.38 MiB  188.11 MiB/s 4s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "# copy files\n",
    "!mc mb minio/minio-seldon -p\n",
    "!mc cp -r ./models minio/minio-seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile secret.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: seldon-init-container-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  RCLONE_CONFIG_S3_TYPE: s3\n",
    "  RCLONE_CONFIG_S3_PROVIDER: minio\n",
    "  RCLONE_CONFIG_S3_ENV_AUTH: \"false\"\n",
    "  RCLONE_CONFIG_S3_ACCESS_KEY_ID: minioadmin\n",
    "  RCLONE_CONFIG_S3_SECRET_ACCESS_KEY: minioadmin\n",
    "  RCLONE_CONFIG_S3_ENDPOINT: http://minio.minio-system.svc.cluster.local:9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting seldon-triton.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile seldon-triton.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: image-models\n",
    "spec:\n",
    "  name: default\n",
    "  predictors:\n",
    "  - graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: s3://minio-seldon/models\n",
    "      envSecretRefName: seldon-init-container-secret\n",
    "      name: image-models # This should have the same name as the model inside\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/seldon-init-container-secret configured\n",
      "seldondeployment.machinelearning.seldon.io/image-models created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f secret.yaml -n default\n",
    "!kubectl apply -f seldon-triton.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:32000/seldon/default/resnet/v2/models/resnet | jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connectionpool.py:394\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    396\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connection.py:234\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    233\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 234\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1050\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         chunk \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunk)\u001b[39m:\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m chunk \\\n\u001b[1;32m   1049\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(chunk)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[39m# end chunked transfer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:972\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock\u001b[39m.\u001b[39;49msendall(data)\n\u001b[1;32m    973\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connectionpool.py:755\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    753\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 755\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    756\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    757\u001b[0m )\n\u001b[1;32m    758\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/util/retry.py:532\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    533\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    700\u001b[0m     conn,\n\u001b[1;32m    701\u001b[0m     method,\n\u001b[1;32m    702\u001b[0m     url,\n\u001b[1;32m    703\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    704\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    705\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    706\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    707\u001b[0m )\n\u001b[1;32m    709\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connectionpool.py:394\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    396\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/urllib3/connection.py:234\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    233\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 234\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:1050\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         chunk \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunk)\u001b[39m:\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m chunk \\\n\u001b[1;32m   1049\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(chunk)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[39m# end chunked transfer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/http/client.py:972\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock\u001b[39m.\u001b[39;49msendall(data)\n\u001b[1;32m    973\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=25'>26</a>\u001b[0m     output \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39margmax(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m predictions]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=27'>28</a>\u001b[0m triton_seldon_output \u001b[39m=\u001b[39m predict(batch\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=28'>29</a>\u001b[0m triton_seldon_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(classes)[triton_seldon_output]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(triton_seldon_classes)\n",
      "\u001b[1;32m/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb Cell 29\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(data):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=11'>12</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=12'>13</a>\u001b[0m             {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=18'>19</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=19'>20</a>\u001b[0m     }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=21'>22</a>\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mURL\u001b[39m}\u001b[39;49;00m\u001b[39m/v2/models/versions/1/xception/infer\u001b[39;49m\u001b[39m\"\u001b[39;49m, json\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=22'>23</a>\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(r\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=23'>24</a>\u001b[0m         r\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=24'>25</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/infernece-pipeline-joint-optimization/pipelines/15-triton-operations/1-triton-fixed.ipynb#ch0000028vscode-remote?line=25'>26</a>\u001b[0m     output \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39margmax(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m predictions]\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/central/lib/python3.8/site-packages/requests/adapters.py:547\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    549\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    551\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "import json\n",
    " # from subprocess import PIPE, Popen, run\n",
    "import requests\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "URL = \"http://localhost:32000/seldon/default/image-models\"\n",
    " \n",
    " \n",
    "def predict(data):\n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input\",\n",
    "                \"data\": data.tolist(),\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"shape\": data.shape,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    r = requests.post(f\"{URL}/v2/models/versions/1/xception/infer\", json=data)\n",
    "    predictions = np.array(r.json()[\"outputs\"][0][\"data\"]).reshape(\n",
    "        r.json()[\"outputs\"][0][\"shape\"]\n",
    "    )\n",
    "    output = [np.argmax(x) for x in predictions]\n",
    "    return output\n",
    "triton_seldon_output = predict(batch.numpy())\n",
    "triton_seldon_classes = np.array(classes)[triton_seldon_output]\n",
    "print(triton_seldon_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
